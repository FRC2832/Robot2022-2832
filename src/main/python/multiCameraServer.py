#!/usr/bin/env python3

# Copyright (c) FIRST and other WPILib contributors.
# Open Source Software; you can modify and/or share it under the terms of
# the WPILib BSD license file in the root directory of this project.

import json
import time
import sys
import cv2
import numpy as np
import math
import threading
import serial


from cscore import CameraServer, VideoSource, UsbCamera, MjpegServer
from networktables import NetworkTablesInstance, NetworkTables
from enum import Enum

DebugLevel = "info"# debug info warn except

def debug(level, printText):
    global DebugLevel
    if DebugLevel == 'debug' and (level == 'debug' or level == 'info' or level == 'warn' or level == 'except'):
        print(str(time.time()) + ' [' + level + ']: ' + printText)
    elif DebugLevel == 'info' and (level == 'info' or level == 'warn' or level == 'except'):
        print(str(time.time()) + ' [' + level + ']: ' + printText)
    elif DebugLevel == 'warn' and (level == 'warn' or level == 'except'):
        print(str(time.time()) + ' [' + level + ']: ' + printText)
    elif DebugLevel == 'except' and level == 'except':
        print(str(time.time()) + ' [' + level + ']: ' + printText)
        
class DriverCam(threading.Thread):
    def __init__(self):
        threading.Thread.__init__(self)
        debug('info', "Driver Cam Start")
        self.running = True
        
    def run(self):
        global inst
        inst = CameraServer.getInstance()
        cap = cv2.VideoCapture(3)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)
        output_stream = inst.putVideo(name='Processed', height = 320, width=240)
        redOffsetX = 25
        redOffsetY = 36
        yelOffsetX = 19
        yelOffsetY = 30
        while self.running:
            ret, frame = cap.read()
            frame = cv2.rotate(frame, cv2.ROTATE_180)
            frame = cv2.line(frame, (160, 0), (160,250), (0,255,0), 1)
            frame = cv2.line(frame, (0, 90), (320,90), (0,255,0), 1)
            
            frame = cv2.line(frame, (160+yelOffsetY , 0), (160+yelOffsetY,250), (0,255,255), 1)
            frame = cv2.line(frame, (160-yelOffsetY, 0), (160-yelOffsetY,250), (0,255,255), 1)
            
            frame = cv2.line(frame, (0, 90+yelOffsetX -15), (320,90+yelOffsetX-15), (0,255,255), 1)
            frame = cv2.line(frame, (0, 90-yelOffsetX ), (320,90-yelOffsetX), (0,255,255), 1)
            
            frame = cv2.line(frame, (160+redOffsetY , 0), (160+redOffsetY,250), (0,0,255), 1)
            frame = cv2.line(frame, (160-redOffsetY, 0), (160-redOffsetY,250), (0,0,255), 1)
            
            frame = cv2.line(frame, (0, 90+redOffsetX -15), (320,90+redOffsetX-15), (0,0,255), 1)
            frame = cv2.line(frame, (0, 90-redOffsetX ), (320,90-redOffsetX), (0,0,255), 1)
            
            output_stream.putFrame(frame)

class Lidar(threading.Thread):
    def __init__(self, ntinst):
        threading.Thread.__init__(self)
        debug('info', "lidar start")
        self.ntinst = ntinst
        self.table = NetworkTables.getTable('lidar')
        self.running = True
        self.ser = serial.Serial("/dev/ttyS0", 115200, timeout=0)  # mini UART serial device
        
    def run(self):
        while self.running:
            if self.ser.in_waiting > 8:
                raw = self.ser.read(9)
                self.ser.reset_input_buffer()
                if raw[0] == 0x59 and raw[1] == 0x59:

                    distance = raw[2] + raw[3]*256
                    debug('debug', "LIDAR RAW: " + str(distance))
                    self.table.putNumber('distance', distance)
            time.sleep(0.001)
        self.ser.close()

class CameraConfig: pass

class CargoPipeline:
    """
    An OpenCV pipeline generated by GRIP.
    """
    
    def __init__(self,pipeline_values):
        """initializes all values to presets or None if need to be set
        """

        self.__rgb_threshold_red = pipeline_values['rgb_red']
        self.__rgb_threshold_green = pipeline_values['rgb_green']
        self.__rgb_threshold_blue = pipeline_values['rgb_blue']

        self.rgb_threshold_output = None


        self.__hsv_threshold_hue = pipeline_values['hsv_hue']
        self.__hsv_threshold_saturation = pipeline_values['hsv_sat']
        self.__hsv_threshold_value = pipeline_values['hsv_value']

        self.hsv_threshold_output = None


        self.__hsl_threshold_hue = pipeline_values['hsl_hue']
        self.__hsl_threshold_saturation = pipeline_values['hsl_sat']
        self.__hsl_threshold_luminance = pipeline_values['hsl_lum']

        self.hsl_threshold_output = None

        self.__cv_bitwise_or_0_src1 = self.hsv_threshold_output
        self.__cv_bitwise_or_0_src2 = self.hsl_threshold_output

        self.cv_bitwise_or_0_output = None

        self.__cv_bitwise_or_1_src1 = self.cv_bitwise_or_0_output
        self.__cv_bitwise_or_1_src2 = self.rgb_threshold_output

        self.cv_bitwise_or_1_output = None

        self.__find_contours_input = self.cv_bitwise_or_1_output
        self.__find_contours_external_only = False

        self.find_contours_output = None

        self.__filter_contours_contours = self.find_contours_output
        self.__filter_contours_min_area = 7000.0
        self.__filter_contours_min_perimeter = 0.0
        self.__filter_contours_min_width = 0.0
        self.__filter_contours_max_width = 1000.0
        self.__filter_contours_min_height = 0.0
        self.__filter_contours_max_height = 1000.0
        self.__filter_contours_solidity = [85.0, 100.0]
        self.__filter_contours_max_vertices = 1000000.0
        self.__filter_contours_min_vertices = 0.0
        self.__filter_contours_min_ratio = 0.7
        self.__filter_contours_max_ratio = 1.5

        self.filter_contours_output = None

        self.__convex_hulls_contours = self.filter_contours_output

        self.convex_hulls_output = None


    def process(self, source0):
        """
        Runs the pipeline and sets all outputs to new values.
        """
        # Step RGB_Threshold0:
        self.__rgb_threshold_input = source0
        (self.rgb_threshold_output) = self.__rgb_threshold(self.__rgb_threshold_input, self.__rgb_threshold_red, self.__rgb_threshold_green, self.__rgb_threshold_blue)

        # Step HSV_Threshold0:
        self.__hsv_threshold_input = source0
        (self.hsv_threshold_output) = self.__hsv_threshold(self.__hsv_threshold_input, self.__hsv_threshold_hue, self.__hsv_threshold_saturation, self.__hsv_threshold_value)

        # Step HSL_Threshold0:
        self.__hsl_threshold_input = source0
        (self.hsl_threshold_output) = self.__hsl_threshold(self.__hsl_threshold_input, self.__hsl_threshold_hue, self.__hsl_threshold_saturation, self.__hsl_threshold_luminance)

        # Step CV_bitwise_or0:
        self.__cv_bitwise_or_0_src1 = self.hsv_threshold_output
        self.__cv_bitwise_or_0_src2 = self.hsl_threshold_output
        (self.cv_bitwise_or_0_output) = self.__cv_bitwise_or(self.__cv_bitwise_or_0_src1, self.__cv_bitwise_or_0_src2)

        # Step CV_bitwise_or1:
        self.__cv_bitwise_or_1_src1 = self.cv_bitwise_or_0_output
        self.__cv_bitwise_or_1_src2 = self.rgb_threshold_output
        (self.cv_bitwise_or_1_output) = self.__cv_bitwise_or(self.__cv_bitwise_or_1_src1, self.__cv_bitwise_or_1_src2)

        # Step Find_Contours0:
        self.__find_contours_input = self.cv_bitwise_or_1_output
        (self.find_contours_output) = self.__find_contours(self.__find_contours_input, self.__find_contours_external_only)

        # Step Filter_Contours0:
        self.__filter_contours_contours = self.find_contours_output
        (self.filter_contours_output) = self.__filter_contours(self.__filter_contours_contours, self.__filter_contours_min_area, self.__filter_contours_min_perimeter, self.__filter_contours_min_width, self.__filter_contours_max_width, self.__filter_contours_min_height, self.__filter_contours_max_height, self.__filter_contours_solidity, self.__filter_contours_max_vertices, self.__filter_contours_min_vertices, self.__filter_contours_min_ratio, self.__filter_contours_max_ratio)

        # Step Convex_Hulls0:
        self.__convex_hulls_contours = self.filter_contours_output
        (self.convex_hulls_output) = self.__convex_hulls(self.__convex_hulls_contours)


    @staticmethod
    def __rgb_threshold(input, red, green, blue):
        """Segment an image based on color ranges.
        Args:
            input: A BGR numpy.ndarray.
            red: A list of two numbers the are the min and max red.
            green: A list of two numbers the are the min and max green.
            blue: A list of two numbers the are the min and max blue.
        Returns:
            A black and white numpy.ndarray.
        """
        out = cv2.cvtColor(input, cv2.COLOR_BGR2RGB)
        return cv2.inRange(out, (red[0], green[0], blue[0]),  (red[1], green[1], blue[1]))

    @staticmethod
    def __hsv_threshold(input, hue, sat, val):
        """Segment an image based on hue, saturation, and value ranges.
        Args:
            input: A BGR numpy.ndarray.
            hue: A list of two numbers the are the min and max hue.
            sat: A list of two numbers the are the min and max saturation.
            lum: A list of two numbers the are the min and max value.
        Returns:
            A black and white numpy.ndarray.
        """
        out = cv2.cvtColor(input, cv2.COLOR_BGR2HSV)
        return cv2.inRange(out, (hue[0], sat[0], val[0]),  (hue[1], sat[1], val[1]))

    @staticmethod
    def __hsl_threshold(input, hue, sat, lum):
        """Segment an image based on hue, saturation, and luminance ranges.
        Args:
            input: A BGR numpy.ndarray.
            hue: A list of two numbers the are the min and max hue.
            sat: A list of two numbers the are the min and max saturation.
            lum: A list of two numbers the are the min and max luminance.
        Returns:
            A black and white numpy.ndarray.
        """
        out = cv2.cvtColor(input, cv2.COLOR_BGR2HLS)
        return cv2.inRange(out, (hue[0], lum[0], sat[0]),  (hue[1], lum[1], sat[1]))

    @staticmethod
    def __cv_bitwise_or(src1, src2):
        """Computes the per channel or of two images.
        Args:
            src1: A numpy.ndarray.
            src2: A numpy.ndarray.
        Returns:
            A numpy.ndarray the or of the two mats.
        """
        return cv2.bitwise_or(src1, src2)

    @staticmethod
    def __find_contours(input, external_only):
        """Sets the values of pixels in a binary image to their distance to the nearest black pixel.
        Args:
            input: A numpy.ndarray.
            external_only: A boolean. If true only external contours are found.
        Return:
            A list of numpy.ndarray where each one represents a contour.
        """
        if(external_only):
            mode = cv2.RETR_EXTERNAL
        else:
            mode = cv2.RETR_LIST
        method = cv2.CHAIN_APPROX_SIMPLE
        contours, hierarchy =cv2.findContours(input, mode=mode, method=method)
        return contours

    @staticmethod
    def __filter_contours(input_contours, min_area, min_perimeter, min_width, max_width,
                        min_height, max_height, solidity, max_vertex_count, min_vertex_count,
                        min_ratio, max_ratio):
        """Filters out contours that do not meet certain criteria.
        Args:
            input_contours: Contours as a list of numpy.ndarray.
            min_area: The minimum area of a contour that will be kept.
            min_perimeter: The minimum perimeter of a contour that will be kept.
            min_width: Minimum width of a contour.
            max_width: MaxWidth maximum width.
            min_height: Minimum height.
            max_height: Maximimum height.
            solidity: The minimum and maximum solidity of a contour.
            min_vertex_count: Minimum vertex Count of the contours.
            max_vertex_count: Maximum vertex Count.
            min_ratio: Minimum ratio of width to height.
            max_ratio: Maximum ratio of width to height.
        Returns:
            Contours as a list of numpy.ndarray.
        """
        output = []
        for contour in input_contours:
            x,y,w,h = cv2.boundingRect(contour)
            if (w < min_width or w > max_width):
                continue
            if (h < min_height or h > max_height):
                continue
            area = cv2.contourArea(contour)
            if (area < min_area):
                continue
            if (cv2.arcLength(contour, True) < min_perimeter):
                continue
            hull = cv2.convexHull(contour)
            solid = 100 * area / cv2.contourArea(hull)
            if (solid < solidity[0] or solid > solidity[1]):
                continue
            if (len(contour) < min_vertex_count or len(contour) > max_vertex_count):
                continue
            ratio = (float)(w) / h
            if (ratio < min_ratio or ratio > max_ratio):
                continue
            output.append(contour)
        return output

    @staticmethod
    def __convex_hulls(input_contours):
        """Computes the convex hulls of contours.
        Args:
            input_contours: A list of numpy.ndarray that each represent a contour.
        Returns:
            A list of numpy.ndarray that each represent a contour.
        """
        output = []
        for contour in input_contours:
            output.append(cv2.convexHull(contour))
        return output

class TargetPipeline:
    """
    An OpenCV pipeline generated by GRIP.
    """
    
    def __init__(self):
        """initializes all values to presets or None if need to be set
        """

        self.__hsv_threshold_hue = [40.46762589928056, 86.31399317406144]
        self.__hsv_threshold_saturation = [149.05575539568343, 255.0]
        self.__hsv_threshold_value = [48.156474820143885, 255.0]

        self.hsv_threshold_output = None

        self.__find_contours_input = self.hsv_threshold_output
        self.__find_contours_external_only = False

        self.find_contours_output = None

        self.__filter_contours_contours = self.find_contours_output
        self.__filter_contours_min_area = 10.0
        self.__filter_contours_min_perimeter = 0.0
        self.__filter_contours_min_width = 0.0
        self.__filter_contours_max_width = 1000.0
        self.__filter_contours_min_height = 0.0
        self.__filter_contours_max_height = 1000.0
        self.__filter_contours_solidity = [0.0, 100]
        self.__filter_contours_max_vertices = 1000000.0
        self.__filter_contours_min_vertices = 0.0
        self.__filter_contours_min_ratio = 0.0
        self.__filter_contours_max_ratio = 1000.0

        self.filter_contours_output = None


    def process(self, source0):
        """
        Runs the pipeline and sets all outputs to new values.
        """
        # Step HSV_Threshold0:
        self.__hsv_threshold_input = source0
        (self.hsv_threshold_output) = self.__hsv_threshold(self.__hsv_threshold_input, self.__hsv_threshold_hue, self.__hsv_threshold_saturation, self.__hsv_threshold_value)

        # Step Find_Contours0:
        self.__find_contours_input = self.hsv_threshold_output
        (self.find_contours_output) = self.__find_contours(self.__find_contours_input, self.__find_contours_external_only)

        # Step Filter_Contours0:
        self.__filter_contours_contours = self.find_contours_output
        (self.filter_contours_output) = self.__filter_contours(self.__filter_contours_contours, self.__filter_contours_min_area, self.__filter_contours_min_perimeter, self.__filter_contours_min_width, self.__filter_contours_max_width, self.__filter_contours_min_height, self.__filter_contours_max_height, self.__filter_contours_solidity, self.__filter_contours_max_vertices, self.__filter_contours_min_vertices, self.__filter_contours_min_ratio, self.__filter_contours_max_ratio)


    @staticmethod
    def __hsv_threshold(input, hue, sat, val):
        """Segment an image based on hue, saturation, and value ranges.
        Args:
            input: A BGR numpy.ndarray.
            hue: A list of two numbers the are the min and max hue.
            sat: A list of two numbers the are the min and max saturation.
            lum: A list of two numbers the are the min and max value.
        Returns:
            A black and white numpy.ndarray.
        """
        out = cv2.cvtColor(input, cv2.COLOR_BGR2HSV)
        return cv2.inRange(out, (hue[0], sat[0], val[0]),  (hue[1], sat[1], val[1]))

    @staticmethod
    def __find_contours(input, external_only):
        """Sets the values of pixels in a binary image to their distance to the nearest black pixel.
        Args:
            input: A numpy.ndarray.
            external_only: A boolean. If true only external contours are found.
        Return:
            A list of numpy.ndarray where each one represents a contour.
        """
        if(external_only):
            mode = cv2.RETR_EXTERNAL
        else:
            mode = cv2.RETR_LIST
        method = cv2.CHAIN_APPROX_SIMPLE
        contours, hierarchy =cv2.findContours(input, mode=mode, method=method)
        return contours

    @staticmethod
    def __filter_contours(input_contours, min_area, min_perimeter, min_width, max_width,
                        min_height, max_height, solidity, max_vertex_count, min_vertex_count,
                        min_ratio, max_ratio):
        """Filters out contours that do not meet certain criteria.
        Args:
            input_contours: Contours as a list of numpy.ndarray.
            min_area: The minimum area of a contour that will be kept.
            min_perimeter: The minimum perimeter of a contour that will be kept.
            min_width: Minimum width of a contour.
            max_width: MaxWidth maximum width.
            min_height: Minimum height.
            max_height: Maximimum height.
            solidity: The minimum and maximum solidity of a contour.
            min_vertex_count: Minimum vertex Count of the contours.
            max_vertex_count: Maximum vertex Count.
            min_ratio: Minimum ratio of width to height.
            max_ratio: Maximum ratio of width to height.
        Returns:
            Contours as a list of numpy.ndarray.
        """
        output = []
        for contour in input_contours:
            x,y,w,h = cv2.boundingRect(contour)
            if (w < min_width or w > max_width):
                continue
            if (h < min_height or h > max_height):
                continue
            area = cv2.contourArea(contour)
            if (area < min_area):
                continue
            if (cv2.arcLength(contour, True) < min_perimeter):
                continue
            hull = cv2.convexHull(contour)
            solid = 100 * area / cv2.contourArea(hull)
            if (solid < solidity[0] or solid > solidity[1]):
                continue
            if (len(contour) < min_vertex_count or len(contour) > max_vertex_count):
                continue
            ratio = (float)(w) / h
            if (ratio < min_ratio or ratio > max_ratio):
                continue
            output.append(contour)
        return output



def connectionListener(connected, info):
    print(info, '; Connected=%s' % connected)
    with cond:
        notified[0] = True
        cond.notify()

def extra_cargo_processing(pipeline):
    """
        Performs extra processing on the pipeline's outputs and publishes data to NetworkTables.
        :param pipeline: the pipeline that just processed an image
        :return: None
        """
    cargo_x_positions = []
    cargo_y_positions = []

     # Find the bounding boxes of the contours to get x, y, width, and height
    for contour in pipeline.convex_hulls_output:
        x, y, w, h = cv2.boundingRect(contour)
        cargo_x_positions.append(x + w / 2)
        cargo_y_positions.append(y + (h / 2))

    print('cargo x', cargo_x_positions)
    print('cargo y', cargo_y_positions)
    # Publish to the '/vision/red_areas' network table
    table = NetworkTables.getTable('vision')
    table.putNumberArray('cargoX', cargo_x_positions)
    table.putNumberArray('cargoY', cargo_y_positions)

def extra_target_processing(pipeline):
    """
        Performs extra processing on the pipeline's outputs and publishes data to NetworkTables.
        :param pipeline: the pipeline that just processed an image
        :return: None
        """
    target_x_positions = []
    target_y_positions = []
    target_widths = []
    target_heights = []
    target_areas = []

    # Find the bounding boxes of the contours to get x, y, width, and height
    for contour in pipeline.filter_contours_output:
        x, y, w, h = cv2.boundingRect(contour)
        target_x_positions.append(x + w / 2)
        target_y_positions.append(y + (h / 2))
        target_widths.append(w)
        target_heights.append(h)
        target_areas.append(w * h)

    print('center target x', target_x_positions)
    print('center target y', target_y_positions)
    print('target width', target_widths)
    print('target height', target_heights)
    print('target area', target_areas)

    # i know this looks disgusting but it works. A for loop would be neater but would it affect the 20ms loop?
    if len(target_x_positions) == len(target_y_positions) and len(target_x_positions) == len(target_widths) and len(target_x_positions) == len(target_heights) and len(target_x_positions) == len(target_areas): 
        # Publish to the '/vision/red_areas' network table
        table = NetworkTables.getTable('vision')
        table.putNumberArray('targetX', target_x_positions)
        table.putNumberArray('targetY', target_y_positions)
        table.putNumberArray('targetWidth', target_widths)
        table.putNumberArray('targetHeight', target_heights)
        table.putNumberArray('targetArea', target_areas)



team = None
server = False
cameraConfigs = []
switchedCameraConfigs = []
cameras = []


def parseError(str):
    """Report parse error."""
    print("config error in '" + configFile + "': " + str, file=sys.stderr)


def readCameraConfig(config):
    """Read single camera configuration."""
    cam = CameraConfig()

    # name
    try:
        cam.name = config["name"]
    except KeyError:
        parseError("could not read camera name")
        return False

    # path
    try:
        cam.path = config["path"]
    except KeyError:
        parseError("camera '{}': could not read path".format(cam.name))
        return False

    # stream properties
    cam.streamConfig = config.get("stream")

    cam.config = config

    cameraConfigs.append(cam)
    return True


def readSwitchedCameraConfig(config):
    """Read single switched camera configuration."""
    cam = CameraConfig()

    # name
    try:
        cam.name = config["name"]
    except KeyError:
        parseError("could not read switched camera name")
        return False

    # path
    try:
        cam.key = config["key"]
    except KeyError:
        parseError("switched camera '{}': could not read key".format(cam.name))
        return False

    switchedCameraConfigs.append(cam)
    return True


def readConfig():
    """Read configuration file."""
    global team
    global server

    # parse file
    try:
        with open(configFile, "rt", encoding="utf-8") as f:
            j = json.load(f)
    except OSError as err:
        print("could not open '{}': {}".format(configFile, err), file=sys.stderr)
        return False

    # top level must be an object
    if not isinstance(j, dict):
        parseError("must be JSON object")
        return False

    # team number
    try:
        team = j["team"]
    except KeyError:
        parseError("could not read team number")
        return False

    # ntmode (optional)
    if "ntmode" in j:
        str = j["ntmode"]
        if str.lower() == "client":
            server = False
        elif str.lower() == "server":
            server = True
        else:
            parseError("could not understand ntmode value '{}'".format(str))

    # cameras
    try:
        cameras = j["cameras"]
    except KeyError:
        parseError("could not read cameras")
        return False
    for camera in cameras:
        if not readCameraConfig(camera):
            return False

    # switched cameras
    if "switched cameras" in j:
        for camera in j["switched cameras"]:
            if not readSwitchedCameraConfig(camera):
                return False

    return True


def startCamera(config):
    """Start running the camera."""
    print("Starting camera '{}' on {}".format(config.name, config.path))
    global inst
    inst = CameraServer.getInstance()
    camera = UsbCamera(config.name, config.path)
    server = inst.startAutomaticCapture(camera=camera, return_server=True)
    # if not isSinkMade:
    #     cvSink = inst.getVideo()
    #     isSinkMade = True

    camera.setConfigJson(json.dumps(config.config))
    camera.setConnectionStrategy(VideoSource.ConnectionStrategy.kKeepOpen)

    if config.streamConfig is not None:
        server.setConfigJson(json.dumps(config.streamConfig))

    print("Started camera '{}' on {}".format(config.name, config.path))
    return camera


def startSwitchedCamera(config):
    """Start running the switched camera."""
    print("Starting switched camera '{}' on {}".format(config.name, config.key))
    server = CameraServer.getInstance().addSwitchedCamera(config.name)

    def listener(fromobj, key, value, isNew):
        if isinstance(value, float):
            i = int(value)
            if i >= 0 and i < len(cameras):
                server.setSource(cameras[i])
        elif isinstance(value, str):
            for i in range(len(cameraConfigs)):
                if value == cameraConfigs[i].name:
                    server.setSource(cameras[i])
                    break

    NetworkTablesInstance.getDefault().getEntry(config.key).addListener(
        listener,
        NetworkTablesInstance.NotifyFlags.IMMEDIATE |
        NetworkTablesInstance.NotifyFlags.NEW |
        NetworkTablesInstance.NotifyFlags.UPDATE)

    return server

class findTarget (threading.Thread):
    def __init__(self, camera):
        threading.Thread.__init__(self)
        self.cvSink = CameraServer.getInstance().getVideo(camera=camera)

    def run(self):
        target_proc = TargetPipeline()
        img = np.zeros(shape=(240, 320, 3), dtype=np.uint8) 
        while True:
            if self.cvSink is not None:
                ret, img = self.cvSink.grabFrame(img)
            else:
                ret = None
            if ret:
                rawimg = img
                cv2.rectangle(img,(0,400),(640,480),(0,0,0),-1) # draws a black box over the webcam reflection
                target_proc.process(img)
                extra_target_processing(target_proc)
                #output_stream.putFrame(rawimg)

class findCargo (threading.Thread):
    def __init__(self, camera):
        threading.Thread.__init__(self)
        self.cvSink = CameraServer.getInstance().getVideo(camera=camera)

    def run(self):
        red_pipeline = {
            'rgb_red': [218.50282485875704, 255.0], 
            'rgb_green': [0.0, 255.0], 
            'rgb_blue': [0.0, 255.0], 
            'hsv_hue': [0.0, 14.266576622971181], 
            'hsv_sat': [54.60411331951388, 255.0], 
            'hsv_value': [152.56244157216597, 255.0], 
            'hsl_hue': [141.8942811852213, 180.0], 
            'hsl_sat': [36.04286062675284, 255.0], 
            'hsl_lum': [66.82569198878186, 255.0]
        }
        blue_pipeline = {
            'rgb_red': [0.0, 184.54545454545456], 
            'rgb_green': [0.0, 255.0], 
            'rgb_blue': [177.68361581920905, 255.0], 
            'hsv_hue': [86.4406779661017, 121.75320763901397], 
            'hsv_sat': [42.598463601999754, 255.0], 
            'hsv_value': [0.0, 255.0], 
            'hsl_hue': [79.18241677844163, 138.28877005347593], 
            'hsl_sat': [88.86771938381499, 255.0], 
            'hsl_lum': [0.0, 255.0]
        }
        table = NetworkTables.getTable('datatable')
        cargo_proc = CargoPipeline(blue_pipeline)
        img = np.zeros(shape=(240, 320, 3), dtype=np.uint8) 
        lastAlliance = 'blue'
        while True:
            # check if drivers changed alliance
            alliance = table.getString('alliance', 'blue')
            if(alliance != lastAlliance):
                #alliance changed, need to restart the pipeline
                if alliance == 'red':
                    cargo_proc = CargoPipeline(red_pipeline)
                else:
                    cargo_proc = CargoPipeline(blue_pipeline)
                lastAlliance = alliance

            #run the pipeline
            if self.cvSink is not None:
                ret, img = self.cvSink.grabFrame(img)
            else:
                ret = None
            if ret:
                cargo_proc.process(img)
                extra_cargo_processing(cargo_proc)

if __name__ == "__main__":
    cond = threading.Condition()
    notified = [False]
    configFile = "/boot/frc.json"
    cvSink = None
    isSinkMade = False

    if len(sys.argv) >= 2:
        configFile = sys.argv[1]

    # read configuration
    if not readConfig():
        sys.exit(1)

    # start NetworkTables
    ntinst = NetworkTablesInstance.getDefault()
    if server:
        print("Setting up NetworkTables server")
        ntinst.startServer()
    else:
        print("Setting up NetworkTables client for team {}".format(team))
        ntinst.startClientTeam(team)
        ntinst.startDSClient()

    lidarThread = Lidar(ntinst)
    lidarThread.start()
    
    dcamThread = DriverCam()
    dcamThread.start()
    

    # start cameras
    for config in cameraConfigs:
        camera = startCamera(config)
        cameras.append(camera)


        path=camera.getPath()
        if (path=="/dev/video0"):
            threadTarget = findTarget(camera)
            threadTarget.start()
        else:
            threadCargo = findCargo(camera)
            threadCargo.start()

    # start switched cameras
    for config in switchedCameraConfigs:
        startSwitchedCamera(config)

    # the threads from start cameras keep us running forever
